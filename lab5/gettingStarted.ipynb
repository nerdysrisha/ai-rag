{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8162e723",
   "metadata": {},
   "source": [
    "## Getting Started with Azure AI Content Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33599ed",
   "metadata": {},
   "source": [
    "![Azure AI Content Understanding](./Assets/content-understanding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d90bc",
   "metadata": {},
   "source": [
    "### Setting up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c356a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CONTENT_UNDERSTANDING_ENDPOINT = os.getenv(\"CONTENT_UNDERSTANDING_ENDPOINT\").strip().rstrip('/')\n",
    "CONTENT_UNDERSTANDING_API_KEY = os.getenv(\"CONTENT_UNDERSTANDING_API_KEY\")\n",
    "\n",
    "print(\"Endpoint:\", CONTENT_UNDERSTANDING_ENDPOINT)\n",
    "print(\"API Key:\", CONTENT_UNDERSTANDING_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0b173",
   "metadata": {},
   "source": [
    "### Using Prebuilt-Document Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80622884",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt_document_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-documentAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "document_url = \"https://github.com/kuljotSB/RAGwithAzureOpenAI/raw/refs/heads/main/ContentUnderstanding/Samples/invoice.pdf\"\n",
    "\n",
    "body = {\n",
    "    \"url\": document_url\n",
    "}\n",
    "\n",
    "document_analysis_result = {}\n",
    "\n",
    "try:\n",
    "    headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "            }\n",
    "\n",
    "    response = requests.post(prebuilt_document_analyzer_url, headers=headers, json=body)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    analysis_id = result.get(\"id\")\n",
    "    print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "    # Using the analysis ID to get results; polling until the analysis is complete\n",
    "    get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "    }\n",
    "    analysis_status = \"Running\"\n",
    "    while analysis_status == \"Running\":\n",
    "        status_response = requests.get(get_result_url, headers=headers)\n",
    "        status_response.raise_for_status()\n",
    "        status_result = status_response.json()\n",
    "        analysis_status = status_result.get(\"status\")\n",
    "        print(\"Current Analysis Status:\", analysis_status)\n",
    "        if analysis_status == \"Running\":\n",
    "            import time\n",
    "            time.sleep(1)  # Wait before polling again\n",
    "    result_response = requests.get(get_result_url, headers=headers)\n",
    "    result_response.raise_for_status()\n",
    "    document_analysis_result = result_response.json()\n",
    "    print(\"Document Analysis Result:\", document_analysis_result)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfadcbef",
   "metadata": {},
   "source": [
    "### Function to display Document Analyzer results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "def _hr(char=\"─\", width=80):\n",
    "    return char * width\n",
    "\n",
    "def _h(text: str, width=80):\n",
    "    pad = \" \" * 2\n",
    "    line = f\"{pad}{text.strip()} \"\n",
    "    return f\"{_hr('=')} \\n{line}\\n{_hr('=')}\"\n",
    "\n",
    "def _subh(text: str):\n",
    "    return f\"\\n{text}\\n{_hr()}\"\n",
    "\n",
    "def _kv(k: str, v: Any, k_width=22):\n",
    "    k = (k or \"\").strip()\n",
    "    if isinstance(v, (dict, list)):\n",
    "        v_str = json.dumps(v, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        v_str = \"\" if v is None else str(v)\n",
    "    return f\"{k:<{k_width}} : {v_str}\"\n",
    "\n",
    "def _wrap_block(text: str, width=100, indent=\"    \"):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    wrapped = textwrap.fill(text, width=width)\n",
    "    return textwrap.indent(wrapped, indent)\n",
    "\n",
    "def display_document_analyzer_content_understanding_result(\n",
    "    analysis_result: Dict[str, Any],\n",
    "    save_markdown_path: Optional[str] = None,\n",
    "    max_markdown_chars: int = 1200,\n",
    "    width: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-prints Azure Content Understanding 'prebuilt-documentAnalyzer' result\n",
    "    and optionally writes extracted Markdown to a file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    analysis_result : dict\n",
    "        The JSON-decoded response you printed as \"Analysis Result\".\n",
    "    save_markdown_path : str | None\n",
    "        If provided (e.g., 'analysis.md'), concatenated markdown from all contents\n",
    "        will be written to this path.\n",
    "    max_markdown_chars : int\n",
    "        Truncate console preview of markdown to this many characters (file is not truncated).\n",
    "    width : int\n",
    "        Wrap width for console output.\n",
    "    \"\"\"\n",
    "    # top-level\n",
    "    print(_h(\"Content Understanding • Analysis Summary\", width))\n",
    "    print(_kv(\"Analysis ID\", analysis_result.get(\"id\")))\n",
    "    print(_kv(\"Status\", analysis_result.get(\"status\")))\n",
    "\n",
    "    result = (analysis_result or {}).get(\"result\", {})\n",
    "    usage = (analysis_result or {}).get(\"usage\", {})\n",
    "    tokens = usage.get(\"tokens\", {}) if isinstance(usage, dict) else {}\n",
    "\n",
    "    print(_subh(\"Analyzer Info\"))\n",
    "    print(_kv(\"Analyzer ID\", result.get(\"analyzerId\")))\n",
    "    print(_kv(\"API Version\", result.get(\"apiVersion\")))\n",
    "    created_at = result.get(\"createdAt\")\n",
    "    try:\n",
    "        created_at_local = (\n",
    "            datetime.fromisoformat(created_at.replace(\"Z\", \"+00:00\")).astimezone().isoformat()\n",
    "            if created_at else None\n",
    "        )\n",
    "    except Exception:\n",
    "        created_at_local = created_at\n",
    "    print(_kv(\"Created At (UTC)\", created_at))\n",
    "    print(_kv(\"Created At (local)\", created_at_local))\n",
    "    warnings = result.get(\"warnings\") or []\n",
    "    print(_kv(\"Warnings\", f\"{len(warnings)}\"))\n",
    "\n",
    "    print(_subh(\"Usage\"))\n",
    "    # note: the API you showed returns floats for tokens; just print raw\n",
    "    for k in (\"contextualization\", \"input\", \"output\"):\n",
    "        if k in tokens:\n",
    "            print(_kv(f\"Tokens.{k}\", tokens.get(k)))\n",
    "\n",
    "    # contents\n",
    "    contents = result.get(\"contents\") or []\n",
    "    print(_subh(f\"Contents ({len(contents)})\"))\n",
    "\n",
    "    combined_md_parts = []\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        sp = item.get(\"startPageNumber\")\n",
    "        ep = item.get(\"endPageNumber\")\n",
    "        print(_hr())\n",
    "        print(f\"[Content #{idx}] kind={kind}  pages={sp}–{ep}\")\n",
    "\n",
    "        # Fields block (generic)\n",
    "        fields = (item.get(\"fields\") or {})\n",
    "        if fields:\n",
    "            print(\"• Fields:\")\n",
    "            for fname, fval in fields.items():\n",
    "                if isinstance(fval, dict):\n",
    "                    ftype = fval.get(\"type\")\n",
    "                    vstr = fval.get(\"valueString\") or fval.get(\"valueNumber\") or fval.get(\"valueBoolean\") or fval.get(\"valueArray\") or fval.get(\"valueObject\")\n",
    "                    # fall back to full dict if none of the canonical keys exist\n",
    "                    if vstr is None:\n",
    "                        vstr = fval\n",
    "                    print(_wrap_block(f\"  - {fname} ({ftype}): {vstr}\", width))\n",
    "                else:\n",
    "                    print(_wrap_block(f\"  - {fname}: {fval}\", width))\n",
    "\n",
    "        # Markdown preview\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "        combined_md_parts.append(md)\n",
    "        preview = md.strip()\n",
    "        preview_trunc = (preview[:max_markdown_chars] + \" … [truncated]\") if len(preview) > max_markdown_chars else preview\n",
    "        print(\"• Markdown Preview:\")\n",
    "        print(_wrap_block(preview_trunc, width))\n",
    "\n",
    "    # Save concatenated markdown if requested\n",
    "    if save_markdown_path:\n",
    "        all_md = \"\\n\\n---\\n\\n\".join(part for part in combined_md_parts if part)\n",
    "        out_path = Path(save_markdown_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        out_path.write_text(all_md, encoding=\"utf-8\")\n",
    "        print(_subh(\"Files\"))\n",
    "        print(_kv(\"Markdown saved\", str(out_path.resolve())))\n",
    "\n",
    "display_document_analyzer_content_understanding_result(document_analysis_result, save_markdown_path=\"document_analysis.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fb282",
   "metadata": {},
   "source": [
    "### Testing Image Analysis using Prebuilt-Image Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e98752",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt_image_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-imageAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "image_url = \"https://github.com/kuljotSB/RAGwithAzureOpenAI/raw/refs/heads/main/ContentUnderstanding/Samples/pieChart.png\"\n",
    "\n",
    "body = {\n",
    "    \"url\": image_url\n",
    "}\n",
    "\n",
    "image_analysis_result = {}\n",
    "\n",
    "try:\n",
    "    headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "            }\n",
    "\n",
    "    response = requests.post(prebuilt_image_analyzer_url, headers=headers, json=body)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    analysis_id = result.get(\"id\")\n",
    "    print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "    # Using the analysis ID to get results; polling until the analysis is complete\n",
    "    get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "    }\n",
    "    analysis_status = \"Running\"\n",
    "    while analysis_status == \"Running\":\n",
    "        status_response = requests.get(get_result_url, headers=headers)\n",
    "        status_response.raise_for_status()\n",
    "        status_result = status_response.json()\n",
    "        analysis_status = status_result.get(\"status\")\n",
    "        print(\"Current Analysis Status:\", analysis_status)\n",
    "        if analysis_status == \"Running\":\n",
    "            import time\n",
    "            time.sleep(1)  # Wait before polling again\n",
    "    result_response = requests.get(get_result_url, headers=headers)\n",
    "    result_response.raise_for_status()\n",
    "    image_analysis_result = result_response.json()\n",
    "    print(\"Image Analysis Result:\", image_analysis_result)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60327e",
   "metadata": {},
   "source": [
    "### Printing Image Analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e362be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "# ---------- small console helpers ----------\n",
    "def _hr(char=\"─\", width=80): return char * width\n",
    "def _h(text: str, width=80): return f\"{_hr('=')} \\n  {text.strip()} \\n{_hr('=')}\"\n",
    "def _subh(text: str): return f\"\\n{text}\\n{_hr()}\"\n",
    "def _kv(k: str, v: Any, k_width=22):\n",
    "    if isinstance(v, (dict, list)):\n",
    "        v_str = json.dumps(v, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        v_str = \"\" if v is None else str(v)\n",
    "    return f\"{k:<{k_width}} : {v_str}\"\n",
    "def _wrap_block(text: str, width=100, indent=\"    \"):\n",
    "    if not text: return \"\"\n",
    "    return textwrap.indent(textwrap.fill(text, width=width), indent)\n",
    "\n",
    "# ---------- field formatting ----------\n",
    "def _extract_value_from_fieldobj(fobj: Dict[str, Any]) -> Tuple[str, Any]:\n",
    "    \"\"\"\n",
    "    From an Azure field object like {\"type\":\"string\",\"valueString\":\"...\"}\n",
    "    return (type, value) using the first present among common value* keys.\n",
    "    \"\"\"\n",
    "    ftype = fobj.get(\"type\") or \"unknown\"\n",
    "    for key in (\"valueString\", \"valueNumber\", \"valueBoolean\", \"valueArray\", \"valueObject\"):\n",
    "        if key in fobj:\n",
    "            return ftype, fobj[key]\n",
    "    return ftype, fobj\n",
    "\n",
    "def _fields_to_markdown(fields: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render a fields dict to Markdown bullets with types and values.\n",
    "    Complex values are JSON in fenced blocks.\n",
    "    \"\"\"\n",
    "    if not fields:\n",
    "        return \"_No fields._\"\n",
    "    lines = []\n",
    "    for fname, fval in fields.items():\n",
    "        if isinstance(fval, dict) and \"type\" in fval:\n",
    "            ftype, v = _extract_value_from_fieldobj(fval)\n",
    "        else:\n",
    "            ftype, v = \"unknown\", fval\n",
    "        if isinstance(v, (dict, list)):\n",
    "            v_md = \"```json\\n\" + json.dumps(v, indent=2, ensure_ascii=False) + \"\\n```\"\n",
    "        else:\n",
    "            v_md = str(v) if v is not None else \"\"\n",
    "        lines.append(f\"- **{fname}** (_{ftype}_): {v_md}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- main function ----------\n",
    "def display_image_analysis_result(\n",
    "    analysis_result: Dict[str, Any],\n",
    "    save_markdown_path: Optional[str] = None,\n",
    "    max_markdown_chars: int = 800,\n",
    "    width: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-prints Azure 'prebuilt-imageAnalyzer' result and optionally writes a\n",
    "    Markdown report that includes BOTH the analyzer markdown AND a Fields section.\n",
    "    \"\"\"\n",
    "    # Console summary\n",
    "    print(_h(\"Content Understanding • Image Analysis Summary\", width))\n",
    "    print(_kv(\"Analysis ID\", analysis_result.get(\"id\")))\n",
    "    print(_kv(\"Status\", analysis_result.get(\"status\")))\n",
    "\n",
    "    result = (analysis_result or {}).get(\"result\", {})\n",
    "    usage = (analysis_result or {}).get(\"usage\", {})\n",
    "    tokens = usage.get(\"tokens\", {}) if isinstance(usage, dict) else {}\n",
    "\n",
    "    print(_subh(\"Analyzer Info\"))\n",
    "    print(_kv(\"Analyzer ID\", result.get(\"analyzerId\")))\n",
    "    print(_kv(\"API Version\", result.get(\"apiVersion\")))\n",
    "    created_at = result.get(\"createdAt\")\n",
    "    try:\n",
    "        created_at_local = (\n",
    "            datetime.fromisoformat(created_at.replace(\"Z\", \"+00:00\")).astimezone().isoformat()\n",
    "            if created_at else None\n",
    "        )\n",
    "    except Exception:\n",
    "        created_at_local = created_at\n",
    "    print(_kv(\"Created At (UTC)\", created_at))\n",
    "    print(_kv(\"Created At (local)\", created_at_local))\n",
    "    warnings = result.get(\"warnings\") or []\n",
    "    print(_kv(\"Warnings\", f\"{len(warnings)}\"))\n",
    "\n",
    "    print(_subh(\"Usage\"))\n",
    "    for k in (\"contextualization\", \"input\", \"output\"):\n",
    "        if k in tokens:\n",
    "            print(_kv(f\"Tokens.{k}\", tokens.get(k)))\n",
    "\n",
    "    contents = result.get(\"contents\") or []\n",
    "    print(_subh(f\"Contents ({len(contents)})\"))\n",
    "\n",
    "    # Prepare Markdown report parts (if requested)\n",
    "    md_parts = []\n",
    "    if save_markdown_path:\n",
    "        md_parts.append(\"# Image Analysis Report\\n\")\n",
    "        md_parts.append(\"## Summary\\n\")\n",
    "        md_parts.append(f\"- **Analysis ID:** `{analysis_result.get('id')}`\")\n",
    "        md_parts.append(f\"- **Status:** `{analysis_result.get('status')}`\")\n",
    "        md_parts.append(f\"- **Analyzer:** `{result.get('analyzerId')}`\")\n",
    "        md_parts.append(f\"- **API Version:** `{result.get('apiVersion')}`\")\n",
    "        md_parts.append(f\"- **Created At (UTC):** `{created_at}`\")\n",
    "        md_parts.append(f\"- **Warnings:** `{len(warnings)}`\\n\")\n",
    "\n",
    "        if tokens:\n",
    "            md_parts.append(\"## Usage\\n\")\n",
    "            for k in (\"contextualization\", \"input\", \"output\"):\n",
    "                if k in tokens:\n",
    "                    md_parts.append(f\"- **Tokens.{k}:** `{tokens.get(k)}`\")\n",
    "            md_parts.append(\"\")\n",
    "\n",
    "        md_parts.append(\"## Contents\\n\")\n",
    "\n",
    "    # Iterate content blocks\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        sp = item.get(\"startPageNumber\")\n",
    "        ep = item.get(\"endPageNumber\")\n",
    "        unit = item.get(\"unit\")\n",
    "        pages = item.get(\"pages\")\n",
    "        fields = (item.get(\"fields\") or {})\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "\n",
    "        # Console view\n",
    "        print(_hr())\n",
    "        print(f\"[Content #{idx}] kind={kind}  pages={sp}–{ep}\")\n",
    "        if unit:  print(_kv(\"Unit\", unit))\n",
    "        if pages: print(_kv(\"Pages\", pages))\n",
    "\n",
    "        if fields:\n",
    "            print(\"• Fields:\")\n",
    "            for fname, fval in fields.items():\n",
    "                if isinstance(fval, dict):\n",
    "                    ftype, v = _extract_value_from_fieldobj(fval)\n",
    "                    if v is None: v = fval\n",
    "                    print(_wrap_block(f\"  - {fname} ({ftype}): {v}\", width))\n",
    "                else:\n",
    "                    print(_wrap_block(f\"  - {fname}: {fval}\", width))\n",
    "\n",
    "        preview = md.strip()\n",
    "        preview_trunc = (preview[:max_markdown_chars] + \" … [truncated]\") if len(preview) > max_markdown_chars else preview\n",
    "        print(\"• Markdown Preview:\")\n",
    "        print(_wrap_block(preview_trunc, width))\n",
    "\n",
    "        # Markdown report for this content\n",
    "        if save_markdown_path:\n",
    "            md_parts.append(f\"### Content #{idx}\")\n",
    "            meta_bits = []\n",
    "            if kind: meta_bits.append(f\"**kind:** `{kind}`\")\n",
    "            if sp is not None and ep is not None: meta_bits.append(f\"**pages:** `{sp}–{ep}`\")\n",
    "            if unit: meta_bits.append(f\"**unit:** `{unit}`\")\n",
    "            if meta_bits:\n",
    "                md_parts.append(\"> \" + \" • \".join(meta_bits))\n",
    "            if pages:\n",
    "                md_parts.append(\"<details><summary>Pages metadata</summary>\\n\\n```json\\n\" +\n",
    "                                json.dumps(pages, indent=2, ensure_ascii=False) +\n",
    "                                \"\\n```\\n</details>\\n\")\n",
    "            # Analyzer-provided markdown (exact)\n",
    "            md_parts.append(\"#### Analyzer Markdown\")\n",
    "            md_parts.append(md if md.strip() else \"_(empty)_\")\n",
    "            # Fields as Markdown bullets\n",
    "            md_parts.append(\"#### Fields\")\n",
    "            md_parts.append(_fields_to_markdown(fields))\n",
    "            md_parts.append(\"---\")\n",
    "\n",
    "    # Write Markdown file if requested\n",
    "    if save_markdown_path:\n",
    "        out_path = Path(save_markdown_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        report = \"\\n\".join(md_parts).rstrip() + \"\\n\"\n",
    "        out_path.write_text(report, encoding=\"utf-8\")\n",
    "        print(_subh(\"Files\"))\n",
    "        print(_kv(\"Markdown saved\", str(out_path.resolve())))\n",
    "\n",
    "display_image_analysis_result(image_analysis_result, save_markdown_path=\"image_analysis.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a786e1",
   "metadata": {},
   "source": [
    "### Testing Audio Analysis using Prebuilt-Audio Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b988db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt_audio_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-audioAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "audio_url = \"https://github.com/kuljotSB/RAGwithAzureOpenAI/raw/refs/heads/main/ContentUnderstanding/Samples/audio.wav\"\n",
    "\n",
    "body = {\n",
    "    \"url\": audio_url\n",
    "}\n",
    "\n",
    "audio_analysis_result = {}\n",
    "\n",
    "try:\n",
    "    headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "            }\n",
    "\n",
    "    response = requests.post(prebuilt_audio_analyzer_url, headers=headers, json=body)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    analysis_id = result.get(\"id\")\n",
    "    print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "    # Using the analysis ID to get results; polling until the analysis is complete\n",
    "    get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "    }\n",
    "    analysis_status = \"Running\"\n",
    "    while analysis_status == \"Running\":\n",
    "        status_response = requests.get(get_result_url, headers=headers)\n",
    "        status_response.raise_for_status()\n",
    "        status_result = status_response.json()\n",
    "        analysis_status = status_result.get(\"status\")\n",
    "        print(\"Current Analysis Status:\", analysis_status)\n",
    "        if analysis_status == \"Running\":\n",
    "            import time\n",
    "            time.sleep(3)  # Wait before polling again\n",
    "    result_response = requests.get(get_result_url, headers=headers)\n",
    "    result_response.raise_for_status()\n",
    "    audio_analysis_result = result_response.json()\n",
    "    print(\"Audio Analysis Result:\", audio_analysis_result)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31521898",
   "metadata": {},
   "source": [
    "### Printing Audio Analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4333f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "\n",
    "# ---------- small console helpers ----------\n",
    "def _hr(char=\"─\", width=80): return char * width\n",
    "def _h(text: str, width=80): return f\"{_hr('=')} \\n  {text.strip()} \\n{_hr('=')}\"\n",
    "def _subh(text: str): return f\"\\n{text}\\n{_hr()}\"\n",
    "def _kv(k: str, v: Any, k_width=24):\n",
    "    if isinstance(v, (dict, list)):\n",
    "        v_str = json.dumps(v, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        v_str = \"\" if v is None else str(v)\n",
    "    return f\"{k:<{k_width}} : {v_str}\"\n",
    "def _wrap_block(text: str, width=100, indent=\"    \"):\n",
    "    if not text: return \"\"\n",
    "    return textwrap.indent(textwrap.fill(text, width=width), indent)\n",
    "\n",
    "# ---------- field formatting ----------\n",
    "def _extract_value_from_fieldobj(fobj: Dict[str, Any]) -> Tuple[str, Any]:\n",
    "    \"\"\"\n",
    "    From an Azure 'field' object like {\"type\":\"string\",\"valueString\":\"...\"}\n",
    "    return (type, value) using the first present among value* keys.\n",
    "    \"\"\"\n",
    "    ftype = fobj.get(\"type\") or \"unknown\"\n",
    "    for key in (\"valueString\", \"valueNumber\", \"valueBoolean\", \"valueArray\", \"valueObject\"):\n",
    "        if key in fobj:\n",
    "            return ftype, fobj[key]\n",
    "    return ftype, fobj\n",
    "\n",
    "def _fields_to_markdown(fields: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render a fields dict to Markdown bullets with types and values.\n",
    "    Complex values are JSON in fenced blocks.\n",
    "    \"\"\"\n",
    "    if not fields:\n",
    "        return \"_No fields._\"\n",
    "    lines = []\n",
    "    for fname, fval in fields.items():\n",
    "        if isinstance(fval, dict) and \"type\" in fval:\n",
    "            ftype, v = _extract_value_from_fieldobj(fval)\n",
    "        else:\n",
    "            ftype, v = \"unknown\", fval\n",
    "        if isinstance(v, (dict, list)):\n",
    "            v_md = \"```json\\n\" + json.dumps(v, indent=2, ensure_ascii=False) + \"\\n```\"\n",
    "        else:\n",
    "            v_md = str(v) if v is not None else \"\"\n",
    "        lines.append(f\"- **{fname}** (_{ftype}_): {v_md}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- audio utilities ----------\n",
    "def _fmt_ms(ms: int) -> str:\n",
    "    \"\"\"Format milliseconds as HH:MM:SS.mmm\"\"\"\n",
    "    if ms is None:\n",
    "        return \"\"\n",
    "    td = timedelta(milliseconds=int(ms))\n",
    "    # Manually format to always show milliseconds\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    seconds = total_seconds % 60\n",
    "    millis = int(ms) % 1000\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{millis:03d}\"\n",
    "\n",
    "def _phrases_to_markdown(phrases: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Render transcriptPhrases into a compact Markdown list with timestamps, speaker, confidence, and text.\n",
    "    \"\"\"\n",
    "    if not phrases:\n",
    "        return \"_No transcript phrases._\"\n",
    "    out = []\n",
    "    for p in phrases:\n",
    "        spk = p.get(\"speaker\") or \"Speaker\"\n",
    "        st = _fmt_ms(p.get(\"startTimeMs\"))\n",
    "        et = _fmt_ms(p.get(\"endTimeMs\"))\n",
    "        conf = p.get(\"confidence\")\n",
    "        txt = p.get(\"text\") or \"\"\n",
    "        conf_str = f\"{conf:.3f}\" if isinstance(conf, (int, float)) else str(conf) if conf is not None else \"\"\n",
    "        out.append(f\"- `{st} → {et}` **{spk}** (_conf: {conf_str}_): {txt}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "# ---------- main function ----------\n",
    "def display_audio_analysis_result(\n",
    "    analysis_result: Dict[str, Any],\n",
    "    save_markdown_path: Optional[str] = None,\n",
    "    max_markdown_chars: int = 1600,\n",
    "    width: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-prints Azure 'prebuilt-audioAnalyzer' result and optionally writes a\n",
    "    Markdown report that includes BOTH the analyzer markdown AND a Fields section,\n",
    "    plus a compact transcript phrases section.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    analysis_result : dict\n",
    "        The JSON-decoded response (\"Audio Analysis Result\").\n",
    "    save_markdown_path : str | None\n",
    "        If provided, writes a Markdown report with analyzer markdown, fields, and phrases.\n",
    "    max_markdown_chars : int\n",
    "        Truncate console preview of analyzer markdown to this many characters.\n",
    "    width : int\n",
    "        Wrap width for console output.\n",
    "    \"\"\"\n",
    "    # Console summary\n",
    "    print(_h(\"Content Understanding • Audio Analysis Summary\", width))\n",
    "    print(_kv(\"Analysis ID\", analysis_result.get(\"id\")))\n",
    "    print(_kv(\"Status\", analysis_result.get(\"status\")))\n",
    "\n",
    "    result = (analysis_result or {}).get(\"result\", {})\n",
    "    usage = (analysis_result or {}).get(\"usage\", {}) or {}\n",
    "    tokens = usage.get(\"tokens\", {}) if isinstance(usage, dict) else {}\n",
    "    audio_hours = usage.get(\"audioHours\")\n",
    "\n",
    "    print(_subh(\"Analyzer Info\"))\n",
    "    print(_kv(\"Analyzer ID\", result.get(\"analyzerId\")))\n",
    "    print(_kv(\"API Version\", result.get(\"apiVersion\")))\n",
    "    print(_kv(\"String Encoding\", result.get(\"stringEncoding\")))\n",
    "    created_at = result.get(\"createdAt\")\n",
    "    try:\n",
    "        created_at_local = (\n",
    "            datetime.fromisoformat(created_at.replace(\"Z\", \"+00:00\")).astimezone().isoformat()\n",
    "            if created_at else None\n",
    "        )\n",
    "    except Exception:\n",
    "        created_at_local = created_at\n",
    "    print(_kv(\"Created At (UTC)\", created_at))\n",
    "    print(_kv(\"Created At (local)\", created_at_local))\n",
    "    warnings = result.get(\"warnings\") or []\n",
    "    print(_kv(\"Warnings\", f\"{len(warnings)}\"))\n",
    "\n",
    "    print(_subh(\"Usage\"))\n",
    "    if audio_hours is not None:\n",
    "        print(_kv(\"Audio Hours\", audio_hours))\n",
    "    for k in (\"contextualization\", \"input\", \"output\"):\n",
    "        if k in tokens:\n",
    "            print(_kv(f\"Tokens.{k}\", tokens.get(k)))\n",
    "\n",
    "    contents = result.get(\"contents\") or []\n",
    "    print(_subh(f\"Contents ({len(contents)})\"))\n",
    "\n",
    "    # Prepare Markdown report parts (if requested)\n",
    "    md_parts = []\n",
    "    if save_markdown_path:\n",
    "        md_parts.append(\"# Audio Analysis Report\\n\")\n",
    "        md_parts.append(\"## Summary\\n\")\n",
    "        md_parts.append(f\"- **Analysis ID:** `{analysis_result.get('id')}`\")\n",
    "        md_parts.append(f\"- **Status:** `{analysis_result.get('status')}`\")\n",
    "        md_parts.append(f\"- **Analyzer:** `{result.get('analyzerId')}`\")\n",
    "        md_parts.append(f\"- **API Version:** `{result.get('apiVersion')}`\")\n",
    "        md_parts.append(f\"- **String Encoding:** `{result.get('stringEncoding')}`\")\n",
    "        md_parts.append(f\"- **Created At (UTC):** `{created_at}`\")\n",
    "        md_parts.append(f\"- **Warnings:** `{len(warnings)}`\\n\")\n",
    "        if audio_hours is not None:\n",
    "            md_parts.append(f\"- **Audio Hours:** `{audio_hours}`\")\n",
    "        if tokens:\n",
    "            md_parts.append(\"\\n## Usage\")\n",
    "            for k in (\"contextualization\", \"input\", \"output\"):\n",
    "                if k in tokens:\n",
    "                    md_parts.append(f\"- **Tokens.{k}:** `{tokens.get(k)}`\")\n",
    "        md_parts.append(\"\\n## Contents\\n\")\n",
    "\n",
    "    # Iterate content blocks\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        start_ms = item.get(\"startTimeMs\")\n",
    "        end_ms = item.get(\"endTimeMs\")\n",
    "        fields = (item.get(\"fields\") or {})\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "        phrases = item.get(\"transcriptPhrases\") or []\n",
    "\n",
    "        # Console view\n",
    "        print(_hr())\n",
    "        print(f\"[Content #{idx}] kind={kind}  time={_fmt_ms(start_ms)}–{_fmt_ms(end_ms)}\")\n",
    "        if fields:\n",
    "            print(\"• Fields:\")\n",
    "            for fname, fval in fields.items():\n",
    "                if isinstance(fval, dict):\n",
    "                    ftype, v = _extract_value_from_fieldobj(fval)\n",
    "                    if v is None: v = fval\n",
    "                    print(_wrap_block(f\"  - {fname} ({ftype}): {v}\", width))\n",
    "                else:\n",
    "                    print(_wrap_block(f\"  - {fname}: {fval}\", width))\n",
    "\n",
    "        preview = md.strip()\n",
    "        preview_trunc = (preview[:max_markdown_chars] + \" … [truncated]\") if len(preview) > max_markdown_chars else preview\n",
    "        print(\"• Analyzer Markdown Preview:\")\n",
    "        print(_wrap_block(preview_trunc, width))\n",
    "\n",
    "        if phrases:\n",
    "            print(\"• Transcript Phrases (compact):\")\n",
    "            print(_wrap_block(_phrases_to_markdown(phrases), width))\n",
    "\n",
    "        # Markdown report for this content\n",
    "        if save_markdown_path:\n",
    "            md_parts.append(f\"### Content #{idx}\")\n",
    "            meta_bits = []\n",
    "            if kind: meta_bits.append(f\"**kind:** `{kind}`\")\n",
    "            if start_ms is not None and end_ms is not None:\n",
    "                meta_bits.append(f\"**time:** `{_fmt_ms(start_ms)}–{_fmt_ms(end_ms)}`\")\n",
    "            if meta_bits:\n",
    "                md_parts.append(\"> \" + \" • \".join(meta_bits))\n",
    "\n",
    "            # Analyzer-provided markdown (exact)\n",
    "            md_parts.append(\"#### Analyzer Markdown\")\n",
    "            md_parts.append(md if md.strip() else \"_(empty)_\")\n",
    "\n",
    "            # Fields\n",
    "            md_parts.append(\"#### Fields\")\n",
    "            md_parts.append(_fields_to_markdown(fields))\n",
    "\n",
    "            # Transcript phrases (compact bullets)\n",
    "            md_parts.append(\"#### Transcript Phrases\")\n",
    "            md_parts.append(_phrases_to_markdown(phrases))\n",
    "\n",
    "            md_parts.append(\"---\")\n",
    "\n",
    "    # Write Markdown file if requested\n",
    "    if save_markdown_path:\n",
    "        out_path = Path(save_markdown_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        report = \"\\n\".join(md_parts).rstrip() + \"\\n\"\n",
    "        out_path.write_text(report, encoding=\"utf-8\")\n",
    "        print(_subh(\"Files\"))\n",
    "        print(_kv(\"Markdown saved\", str(out_path.resolve())))\n",
    "\n",
    "display_audio_analysis_result(audio_analysis_result, save_markdown_path=\"audio_analysis.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552abc4",
   "metadata": {},
   "source": [
    "### Testing Video Analysis using Prebuilt-Video Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt_video_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-videoAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "video_url = \"https://github.com/kuljotSB/RAGwithAzureOpenAI/raw/refs/heads/main/ContentUnderstanding/Samples/FlightSimulator.mp4\"\n",
    "\n",
    "body = {\n",
    "    \"url\": video_url\n",
    "}\n",
    "\n",
    "video_analysis_result = {}\n",
    "\n",
    "try:\n",
    "    headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "            }\n",
    "\n",
    "    response = requests.post(prebuilt_video_analyzer_url, headers=headers, json=body)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    analysis_id = result.get(\"id\")\n",
    "    print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "    # Using the analysis ID to get results; polling until the analysis is complete\n",
    "    get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "    }\n",
    "    analysis_status = \"Running\"\n",
    "    while analysis_status == \"Running\":\n",
    "        status_response = requests.get(get_result_url, headers=headers)\n",
    "        status_response.raise_for_status()\n",
    "        status_result = status_response.json()\n",
    "        analysis_status = status_result.get(\"status\")\n",
    "        print(\"Current Analysis Status:\", analysis_status)\n",
    "        if analysis_status == \"Running\":\n",
    "            import time\n",
    "            time.sleep(30)  # Wait before polling again\n",
    "    result_response = requests.get(get_result_url, headers=headers)\n",
    "    result_response.raise_for_status()\n",
    "    video_analysis_result = result_response.json()\n",
    "    print(\"Video Analysis Result:\", video_analysis_result)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba5cba",
   "metadata": {},
   "source": [
    "### Printing Video Analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e275ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import textwrap\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "\n",
    "# ---------- small console helpers ----------\n",
    "def _hr(char=\"─\", width=80): return char * width\n",
    "def _h(text: str, width=80): return f\"{_hr('=')} \\n  {text.strip()} \\n{_hr('=')}\"\n",
    "def _subh(text: str): return f\"\\n{text}\\n{_hr()}\"\n",
    "def _kv(k: str, v: Any, k_width=24):\n",
    "    if isinstance(v, (dict, list)):\n",
    "        v_str = json.dumps(v, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        v_str = \"\" if v is None else str(v)\n",
    "    return f\"{k:<{k_width}} : {v_str}\"\n",
    "def _wrap_block(text: str, width=100, indent=\"    \"):\n",
    "    if not text: return \"\"\n",
    "    return textwrap.indent(textwrap.fill(text, width=width), indent)\n",
    "\n",
    "# ---------- field formatting ----------\n",
    "def _extract_value_from_fieldobj(fobj: Dict[str, Any]) -> Tuple[str, Any]:\n",
    "    \"\"\"\n",
    "    From an Azure field object like {\"type\":\"string\",\"valueString\":\"...\"}\n",
    "    return (type, value) using the first present among value* keys.\n",
    "    \"\"\"\n",
    "    ftype = fobj.get(\"type\") or \"unknown\"\n",
    "    for key in (\"valueString\", \"valueNumber\", \"valueInteger\", \"valueBoolean\", \"valueArray\", \"valueObject\"):\n",
    "        if key in fobj:\n",
    "            return ftype, fobj[key]\n",
    "    return ftype, fobj\n",
    "\n",
    "def _fields_to_markdown(fields: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Render 'fields' dict to Markdown bullets with types and values.\n",
    "    Complex values are JSON in fenced blocks. Handles arrays/objects.\n",
    "    \"\"\"\n",
    "    if not fields:\n",
    "        return \"_No fields._\"\n",
    "    lines = []\n",
    "    for fname, fval in fields.items():\n",
    "        if isinstance(fval, dict) and \"type\" in fval:\n",
    "            ftype, v = _extract_value_from_fieldobj(fval)\n",
    "        else:\n",
    "            ftype, v = \"unknown\", fval\n",
    "        if isinstance(v, (dict, list)):\n",
    "            v_md = \"```json\\n\" + json.dumps(v, indent=2, ensure_ascii=False) + \"\\n```\"\n",
    "        else:\n",
    "            v_md = \"\" if v is None else str(v)\n",
    "        lines.append(f\"- **{fname}** (_{ftype}_): {v_md}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- video/audio utilities ----------\n",
    "def _fmt_ms(ms: Optional[int]) -> str:\n",
    "    \"\"\"Format milliseconds as HH:MM:SS.mmm\"\"\"\n",
    "    if ms is None:\n",
    "        return \"\"\n",
    "    td = timedelta(milliseconds=int(ms))\n",
    "    total_seconds = int(td.total_seconds())\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    seconds = total_seconds % 60\n",
    "    millis = int(ms) % 1000\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}.{millis:03d}\"\n",
    "\n",
    "def _phrases_to_markdown(phrases: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Render transcriptPhrases into a compact Markdown list with timestamps, speaker, confidence, and text.\n",
    "    \"\"\"\n",
    "    if not phrases:\n",
    "        return \"_No transcript phrases._\"\n",
    "    out = []\n",
    "    for p in phrases:\n",
    "        spk = p.get(\"speaker\") or \"Speaker\"\n",
    "        st = _fmt_ms(p.get(\"startTimeMs\"))\n",
    "        et = _fmt_ms(p.get(\"endTimeMs\"))\n",
    "        conf = p.get(\"confidence\")\n",
    "        txt = p.get(\"text\") or \"\"\n",
    "        conf_str = f\"{conf:.3f}\" if isinstance(conf, (int, float)) else (str(conf) if conf is not None else \"\")\n",
    "        out.append(f\"- `{st} → {et}` **{spk}** (_conf: {conf_str}_): {txt}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _segments_to_markdown(segments: List[Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Render plain segment dicts like [{'startTimeMs':.., 'endTimeMs':.., 'description':.., 'segmentId':..}]\n",
    "    \"\"\"\n",
    "    if not segments:\n",
    "        return \"_No segments._\"\n",
    "    out = []\n",
    "    for s in segments:\n",
    "        st = _fmt_ms(s.get(\"startTimeMs\"))\n",
    "        et = _fmt_ms(s.get(\"endTimeMs\"))\n",
    "        desc = s.get(\"description\") or \"\"\n",
    "        sid = s.get(\"segmentId\")\n",
    "        head = f\"- **Segment {sid}**\" if sid else \"- **Segment**\"\n",
    "        out.append(f\"{head} `{st} → {et}`: {desc}\")\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def _times_to_markdown(times_ms: List[int], title: str) -> str:\n",
    "    if not times_ms:\n",
    "        return f\"_No {title.lower()}._\"\n",
    "    return \"\\n\".join(f\"- `{_fmt_ms(t)}`\" for t in times_ms)\n",
    "\n",
    "# ---------- main function ----------\n",
    "def display_video_analysis_result(\n",
    "    analysis_result: Dict[str, Any],\n",
    "    save_markdown_path: Optional[str] = None,\n",
    "    max_markdown_chars: int = 2000,\n",
    "    width: int = 100,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Pretty-prints Azure 'prebuilt-videoAnalyzer' result and optionally writes a\n",
    "    Markdown report that includes analyzer markdown, Fields, segments, keyframes,\n",
    "    camera shots, and transcript phrases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    analysis_result : dict\n",
    "        The JSON-decoded response (\"Video Analysis Result\").\n",
    "    save_markdown_path : str | None\n",
    "        If provided, writes a Markdown report with analyzer markdown, fields, and extras.\n",
    "    max_markdown_chars : int\n",
    "        Truncate console preview of analyzer markdown to this many characters.\n",
    "    width : int\n",
    "        Wrap width for console output.\n",
    "    \"\"\"\n",
    "    # Console summary\n",
    "    print(_h(\"Content Understanding • Video Analysis Summary\", width))\n",
    "    print(_kv(\"Analysis ID\", analysis_result.get(\"id\")))\n",
    "    print(_kv(\"Status\", analysis_result.get(\"status\")))\n",
    "\n",
    "    result = (analysis_result or {}).get(\"result\", {})\n",
    "    usage = (analysis_result or {}).get(\"usage\", {}) or {}\n",
    "    tokens = usage.get(\"tokens\", {}) if isinstance(usage, dict) else {}\n",
    "    video_hours = usage.get(\"videoHours\")\n",
    "\n",
    "    print(_subh(\"Analyzer Info\"))\n",
    "    print(_kv(\"Analyzer ID\", result.get(\"analyzerId\")))\n",
    "    print(_kv(\"API Version\", result.get(\"apiVersion\")))\n",
    "    print(_kv(\"String Encoding\", result.get(\"stringEncoding\")))\n",
    "    created_at = result.get(\"createdAt\")\n",
    "    try:\n",
    "        created_at_local = (\n",
    "            datetime.fromisoformat(created_at.replace(\"Z\", \"+00:00\")).astimezone().isoformat()\n",
    "            if created_at else None\n",
    "        )\n",
    "    except Exception:\n",
    "        created_at_local = created_at\n",
    "    print(_kv(\"Created At (UTC)\", created_at))\n",
    "    print(_kv(\"Created At (local)\", created_at_local))\n",
    "    warnings = result.get(\"warnings\") or []\n",
    "    print(_kv(\"Warnings\", f\"{len(warnings)}\"))\n",
    "\n",
    "    print(_subh(\"Video Metadata\"))\n",
    "    print(_kv(\"Start Time\", _fmt_ms(result.get(\"startTimeMs\"))))\n",
    "    print(_kv(\"End Time\", _fmt_ms(result.get(\"endTimeMs\"))))\n",
    "    print(_kv(\"Width\", result.get(\"width\")))\n",
    "    print(_kv(\"Height\", result.get(\"height\")))\n",
    "\n",
    "    print(_subh(\"Usage\"))\n",
    "    if video_hours is not None:\n",
    "        print(_kv(\"Video Hours\", video_hours))\n",
    "    for k in (\"contextualization\", \"input\", \"output\"):\n",
    "        if k in tokens:\n",
    "            print(_kv(f\"Tokens.{k}\", tokens.get(k)))\n",
    "\n",
    "    contents = result.get(\"contents\") or []\n",
    "    print(_subh(f\"Contents ({len(contents)})\"))\n",
    "\n",
    "    # Prepare Markdown report parts (if requested)\n",
    "    md_parts = []\n",
    "    if save_markdown_path:\n",
    "        md_parts.append(\"# Video Analysis Report\\n\")\n",
    "        md_parts.append(\"## Summary\\n\")\n",
    "        md_parts.append(f\"- **Analysis ID:** `{analysis_result.get('id')}`\")\n",
    "        md_parts.append(f\"- **Status:** `{analysis_result.get('status')}`\")\n",
    "        md_parts.append(f\"- **Analyzer:** `{result.get('analyzerId')}`\")\n",
    "        md_parts.append(f\"- **API Version:** `{result.get('apiVersion')}`\")\n",
    "        md_parts.append(f\"- **String Encoding:** `{result.get('stringEncoding')}`\")\n",
    "        md_parts.append(f\"- **Created At (UTC):** `{created_at}`\")\n",
    "        md_parts.append(f\"- **Warnings:** `{len(warnings)}`\")\n",
    "        md_parts.append(f\"- **Video Window:** `{_fmt_ms(result.get('startTimeMs'))} → {_fmt_ms(result.get('endTimeMs'))}`\")\n",
    "        md_parts.append(f\"- **Resolution:** `{result.get('width')} x {result.get('height')}`\\n\")\n",
    "        if video_hours is not None:\n",
    "            md_parts.append(f\"- **Video Hours:** `{video_hours}`\")\n",
    "        if tokens:\n",
    "            md_parts.append(\"\\n## Usage\")\n",
    "            for k in (\"contextualization\", \"input\", \"output\"):\n",
    "                if k in tokens:\n",
    "                    md_parts.append(f\"- **Tokens.{k}:** `{tokens.get(k)}`\")\n",
    "        md_parts.append(\"\\n## Contents\\n\")\n",
    "\n",
    "    # Iterate content blocks\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        fields = (item.get(\"fields\") or {})\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "\n",
    "        # Console view\n",
    "        print(_hr())\n",
    "        print(f\"[Content #{idx}] kind={kind}\")\n",
    "\n",
    "        if fields:\n",
    "            print(\"• Fields:\")\n",
    "            for fname, fval in fields.items():\n",
    "                if isinstance(fval, dict):\n",
    "                    ftype, v = _extract_value_from_fieldobj(fval)\n",
    "                    if v is None: v = fval\n",
    "                    print(_wrap_block(f\"  - {fname} ({ftype}): {v}\", width))\n",
    "                else:\n",
    "                    print(_wrap_block(f\"  - {fname}: {fval}\", width))\n",
    "\n",
    "        preview = md.strip()\n",
    "        preview_trunc = (preview[:max_markdown_chars] + \" … [truncated]\") if len(preview) > max_markdown_chars else preview\n",
    "        print(\"• Analyzer Markdown Preview:\")\n",
    "        print(_wrap_block(preview_trunc, width))\n",
    "\n",
    "        # Markdown report for this content\n",
    "        if save_markdown_path:\n",
    "            md_parts.append(f\"### Content #{idx}\")\n",
    "            meta_bits = []\n",
    "            if kind: meta_bits.append(f\"**kind:** `{kind}`\")\n",
    "            if meta_bits:\n",
    "                md_parts.append(\"> \" + \" • \".join(meta_bits))\n",
    "\n",
    "            # Analyzer-provided markdown (exact)\n",
    "            md_parts.append(\"#### Analyzer Markdown\")\n",
    "            md_parts.append(md if md.strip() else \"_(empty)_\")\n",
    "\n",
    "            # Fields\n",
    "            md_parts.append(\"#### Fields\")\n",
    "            md_parts.append(_fields_to_markdown(fields))\n",
    "\n",
    "            md_parts.append(\"---\")\n",
    "\n",
    "    # Extras outside 'contents'\n",
    "    keyframe_times = result.get(\"KeyFrameTimesMs\") or []\n",
    "    camera_shots = result.get(\"cameraShotTimesMs\") or []\n",
    "    plain_segments = result.get(\"segments\") or []\n",
    "    transcript_phrases = result.get(\"transcriptPhrases\") or []\n",
    "\n",
    "    # Console extras\n",
    "    print(_subh(\"Key Frames\"))\n",
    "    if keyframe_times:\n",
    "        print(_wrap_block(_times_to_markdown(keyframe_times, \"Key Frames\"), width))\n",
    "    else:\n",
    "        print(\"_No key frames._\")\n",
    "    print(_subh(\"Camera Shots\"))\n",
    "    if camera_shots:\n",
    "        print(_wrap_block(_times_to_markdown(camera_shots, \"Camera Shots\"), width))\n",
    "    else:\n",
    "        print(\"_No camera shots._\")\n",
    "    print(_subh(\"Segments\"))\n",
    "    print(_wrap_block(_segments_to_markdown(plain_segments), width))\n",
    "    print(_subh(\"Transcript Phrases (compact)\"))\n",
    "    print(_wrap_block(_phrases_to_markdown(transcript_phrases), width))\n",
    "\n",
    "    # Markdown extras\n",
    "    if save_markdown_path:\n",
    "        md_parts.append(\"## Key Frames\")\n",
    "        md_parts.append(_times_to_markdown(keyframe_times, \"Key Frames\"))\n",
    "        md_parts.append(\"\\n## Camera Shots\")\n",
    "        md_parts.append(_times_to_markdown(camera_shots, \"Camera Shots\"))\n",
    "        md_parts.append(\"\\n## Segments\")\n",
    "        md_parts.append(_segments_to_markdown(plain_segments))\n",
    "        md_parts.append(\"\\n## Transcript Phrases\")\n",
    "        md_parts.append(_phrases_to_markdown(transcript_phrases))\n",
    "\n",
    "        out_path = Path(save_markdown_path)\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        report = \"\\n\".join(md_parts).rstrip() + \"\\n\"\n",
    "        out_path.write_text(report, encoding=\"utf-8\")\n",
    "        print(_subh(\"Files\"))\n",
    "        print(_kv(\"Markdown saved\", str(out_path.resolve())))\n",
    "\n",
    "display_video_analysis_result(video_analysis_result, save_markdown_path=\"video_analysis.md\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
