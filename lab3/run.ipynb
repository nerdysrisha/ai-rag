{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Azure AI Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vector_embeddings.svg\" >\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading variables from the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "key = os.getenv(\"AZURE_SEARCH_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "azure_openai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "\n",
    "azure_openai_client = AzureOpenAI(\n",
    "    api_key=azure_openai_key,\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    azure_endpoint=azure_openai_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embedding Generator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(client, text):\n",
    "    embedding_model = os.getenv(\"EMBEDDING_ENGINE\")\n",
    "    \n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model = embedding_model\n",
    "    )\n",
    "    \n",
    "    embeddings=response.model_dump()\n",
    "    return embeddings['data'][0]['embedding']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the review of the creek hotel in Dubai?\"\n",
    "vectorised_user_query = generate_embeddings(azure_openai_client, user_query)\n",
    "print(vectorised_user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending API call to the Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "url = f\"{service_endpoint}/indexes/{index_name}/docs/search?api-version=2023-11-01\"\n",
    "    \n",
    "headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": key\n",
    "    }\n",
    "    \n",
    "body =   {\n",
    "        \"count\": True,\n",
    "        \"select\": \"chunk\",\n",
    "        \"vectorQueries\": [\n",
    "            {\n",
    "                \"vector\": vectorised_user_query,\n",
    "                \"k\": 3,\n",
    "                \"fields\": \"text_vector\",\n",
    "                \"kind\": \"vector\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "response = requests.post(url, headers=headers, data=json.dumps(body))\n",
    "documents = response.json()['value']\n",
    "\n",
    "for doc in documents:\n",
    "    context.append(dict(\n",
    "        {\n",
    "            \"chunk\": doc['chunk'],\n",
    "            \"score\": doc['@search.score']\n",
    "            \n",
    "        }\n",
    "    ))\n",
    "    \n",
    "for doc in context:\n",
    "    print(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling GPT Engine for Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\"You are meant to behave as a RAG chatbot that derives its context from a database of hotel reviews stored in Azure AI Search Solution.\n",
    "please answer strictly from the context from the database provided and if you dont have an answer please politely say so. dont include any extra \n",
    "information that is not in the context and dont include links as well.\n",
    "the context passed to you will be in the form of a pythonic list with each object in the list containing details of hotel reviews and\n",
    "having structure as follows:\n",
    "\n",
    " \"chunk\": \"the content of the review\",\n",
    " \"score\": \"the relevancy score of the review\"\n",
    "\n",
    "\n",
    "the pythonic list contains best 3 matches to the user query based on cosine similarity of the embeddings of the user query and the review descriptions.\n",
    "please structure your answers in a very professional manner and in such a way that the user does not get to know that its RAG working under the hood\n",
    "and its as if they are talking to a human. \"\"\"\n",
    "\n",
    "user_prompt = f\"\"\" the user query is: {user_query}\n",
    "the context is : {context}\"\"\"\n",
    "\n",
    "chat_completions_response = azure_openai_client.chat.completions.create(\n",
    "    model = os.getenv(\"GPT_ENGINE\"),\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(chat_completions_response.choices[0].message.content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
