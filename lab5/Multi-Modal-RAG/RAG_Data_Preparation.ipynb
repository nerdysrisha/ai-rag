{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f551ad6",
   "metadata": {},
   "source": [
    "## Multi-Modal RAG with Azure AI Content Understanding - Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de690c",
   "metadata": {},
   "source": [
    "![rag_data_prep](./Assets/rag_data_prep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b35250",
   "metadata": {},
   "source": [
    "### Install Dependencies and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv pdfminer.six openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e29d30",
   "metadata": {},
   "source": [
    "### Setting Up the Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2277abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Content Understanding Service Connections\n",
    "CONTENT_UNDERSTANDING_ENDPOINT = os.getenv(\"CONTENT_UNDERSTANDING_ENDPOINT\").strip().rstrip('/')\n",
    "CONTENT_UNDERSTANDING_API_KEY = os.getenv(\"CONTENT_UNDERSTANDING_API_KEY\")\n",
    "\n",
    "# Storage Account Connections\n",
    "storage_account_endpoint = os.getenv(\"STORAGE_ACCOUNT_ENDPOINT\").strip().rstrip('/')\n",
    "storage_container_name = os.getenv(\"STORAGE_CONTAINER_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7e30b",
   "metadata": {},
   "source": [
    "### Creating Functions for Performing Video Analysis using Prebuilt Video Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to analyze video using prebuilt video analyzer - makes API calls to Content Understanding service\n",
    "def analyze_video(video_url):\n",
    "    prebuilt_video_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-videoAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "    body = {\n",
    "        \"url\": video_url\n",
    "    }\n",
    "\n",
    "    video_analysis_result = {}\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                    \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "                }\n",
    "\n",
    "        response = requests.post(prebuilt_video_analyzer_url, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        analysis_id = result.get(\"id\")\n",
    "        print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "        # Using the analysis ID to get results; polling until the analysis is complete\n",
    "        get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "        }\n",
    "        analysis_status = \"Running\"\n",
    "        while analysis_status == \"Running\":\n",
    "            status_response = requests.get(get_result_url, headers=headers)\n",
    "            status_response.raise_for_status()\n",
    "            status_result = status_response.json()\n",
    "            analysis_status = status_result.get(\"status\")\n",
    "            print(\"Current Analysis Status:\", analysis_status)\n",
    "            if analysis_status == \"Running\":\n",
    "                import time\n",
    "                time.sleep(20)  # Wait before polling again\n",
    "        result_response = requests.get(get_result_url, headers=headers)\n",
    "        result_response.raise_for_status()\n",
    "        video_analysis_result = result_response.json()\n",
    "        print(\"Video Analysis Result:\", video_analysis_result)\n",
    "        return video_analysis_result\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Creating a Function to extract relevant information from the video analysis result and prepare it for RAG\n",
    "def build_simple_video_dataset(\n",
    "    analysis_result,\n",
    "    video_url,\n",
    "    title,\n",
    "    include_full_transcript_row=True,\n",
    "    include_words=False,          # set True to include word-level timings in each phrase\n",
    "    ensure_ascii=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds dataset rows with segments + their transcripts.\n",
    "    Output rows: [{\"document_title\": ..., \"content_text\": <JSON string>}]\n",
    "\n",
    "    content_text JSON schema (per segment):\n",
    "    {\n",
    "      \"about\": \"...\",\n",
    "      \"video_url\": \"...\",\n",
    "      \"segment_id\": \"...\",\n",
    "      \"time_window\": {\"start_ms\": int, \"end_ms\": int},\n",
    "      \"segment_description\": \"...\",\n",
    "      \"transcript\": [\n",
    "        {\n",
    "          \"speaker\": \"Speaker 1\",\n",
    "          \"start_ms\": 1360,\n",
    "          \"end_ms\": 6640,\n",
    "          \"text\": \"...\",\n",
    "          \"confidence\": 0.937,\n",
    "          \"words\": [ { \"start_ms\": ..., \"end_ms\": ..., \"text\": \"...\" }, ... ]   # only if include_words=True\n",
    "        },\n",
    "        ...\n",
    "      ],\n",
    "      \"transcript_text\": \"Concatenated transcript text for this segment\"\n",
    "    }\n",
    "\n",
    "    If include_full_transcript_row=True, an extra row is appended with the entire transcript.\n",
    "    \"\"\"\n",
    "    result = (analysis_result or {}).get(\"result\", {}) or {}\n",
    "    contents = result.get(\"contents\") or []\n",
    "\n",
    "    # --- Collect segments from both shapes ---\n",
    "    segments = []\n",
    "    for c in contents:\n",
    "        # Shape 1\n",
    "        for s in c.get(\"segments\") or []:\n",
    "            segments.append({\n",
    "                \"segmentId\": s.get(\"segmentId\"),\n",
    "                \"startTimeMs\": s.get(\"startTimeMs\"),\n",
    "                \"endTimeMs\": s.get(\"endTimeMs\"),\n",
    "                \"description\": s.get(\"description\") or \"\"\n",
    "            })\n",
    "        # Shape 2\n",
    "        fields = c.get(\"fields\") or {}\n",
    "        f_segments = ((fields.get(\"Segments\") or {}).get(\"valueArray\")) or []\n",
    "        for item in f_segments:\n",
    "            vo = (item or {}).get(\"valueObject\") or {}\n",
    "            segments.append({\n",
    "                \"segmentId\": ((vo.get(\"SegmentId\") or {}).get(\"valueString\")),\n",
    "                \"startTimeMs\": ((vo.get(\"StartTimeMs\") or {}).get(\"valueInteger\")),\n",
    "                \"endTimeMs\": ((vo.get(\"EndTimeMs\") or {}).get(\"valueInteger\")),\n",
    "                \"description\": ((vo.get(\"Description\") or {}).get(\"valueString\")) or \"\"\n",
    "            })\n",
    "\n",
    "    # --- Collect transcript phrases ---\n",
    "    all_phrases = []\n",
    "    for c in contents:\n",
    "        for p in c.get(\"transcriptPhrases\") or []:\n",
    "            phrase = {\n",
    "                \"speaker\": p.get(\"speaker\"),\n",
    "                \"start_ms\": p.get(\"startTimeMs\"),\n",
    "                \"end_ms\": p.get(\"endTimeMs\"),\n",
    "                \"text\": p.get(\"text\"),\n",
    "                \"confidence\": p.get(\"confidence\")\n",
    "            }\n",
    "            if include_words:\n",
    "                phrase[\"words\"] = [\n",
    "                    {\n",
    "                        \"start_ms\": w.get(\"startTimeMs\"),\n",
    "                        \"end_ms\": w.get(\"endTimeMs\"),\n",
    "                        \"text\": w.get(\"text\")\n",
    "                    } for w in (p.get(\"words\") or [])\n",
    "                ]\n",
    "            all_phrases.append(phrase)\n",
    "\n",
    "    def overlaps(seg_start, seg_end, p_start, p_end):\n",
    "        \"\"\"True if [p_start, p_end] overlaps [seg_start, seg_end].\"\"\"\n",
    "        if seg_start is None or seg_end is None or p_start is None or p_end is None:\n",
    "            return False\n",
    "        return not (p_end < seg_start or p_start > seg_end)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # If no segments, create a single fallback with full transcript (if present)\n",
    "    if not segments:\n",
    "        # Build overall transcript text (if any)\n",
    "        overall_text = \" \".join([p.get(\"text\") or \"\" for p in all_phrases]).strip() if all_phrases else None\n",
    "\n",
    "        content_text_obj = {\n",
    "            \"about\": \"This is a JSON string representing the overall video summary.\",\n",
    "            \"video_url\": video_url,\n",
    "            \"note\": \"No segment details available.\"\n",
    "        }\n",
    "        if overall_text:\n",
    "            content_text_obj[\"full_transcript_text\"] = overall_text\n",
    "            content_text_obj[\"full_transcript\"] = all_phrases  # could be large\n",
    "\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} • Full Video\",\n",
    "            \"content_text\": json.dumps(content_text_obj, ensure_ascii=ensure_ascii)\n",
    "        })\n",
    "        return rows\n",
    "\n",
    "    # Build rows per segment with attached transcript snippets\n",
    "    for s in segments:\n",
    "        st, et = s.get(\"startTimeMs\"), s.get(\"endTimeMs\")\n",
    "        seg_id = s.get(\"segmentId\") or \"?\"\n",
    "        seg_desc = s.get(\"description\") or \"\"\n",
    "\n",
    "        # collect overlapping phrases\n",
    "        seg_phrases = [\n",
    "            p for p in all_phrases\n",
    "            if overlaps(st, et, p.get(\"start_ms\"), p.get(\"end_ms\"))\n",
    "        ]\n",
    "\n",
    "        # Concatenate a readable segment transcript\n",
    "        seg_transcript_text = \" \".join([(p.get(\"text\") or \"\").strip() for p in seg_phrases]).strip()\n",
    "\n",
    "        content_text_obj = {\n",
    "            \"about\": \"This is a JSON string representing a slice of video analysis for RAG.\",\n",
    "            \"video_url\": video_url,\n",
    "            \"segment_id\": seg_id,\n",
    "            \"time_window\": {\"start_ms\": st, \"end_ms\": et},\n",
    "            \"segment_description\": seg_desc,\n",
    "            \"transcript\": seg_phrases,\n",
    "            \"transcript_text\": seg_transcript_text\n",
    "        }\n",
    "\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} • Segment {seg_id}\",\n",
    "            \"content_text\": json.dumps(content_text_obj, ensure_ascii=ensure_ascii)\n",
    "        })\n",
    "\n",
    "    # Optional final row with the full transcript (nice for global search)\n",
    "    if include_full_transcript_row and all_phrases:\n",
    "        full_text = \" \".join([(p.get(\"text\") or \"\").strip() for p in all_phrases]).strip()\n",
    "        full_obj = {\n",
    "            \"about\": \"This is a JSON string representing the overall video transcript.\",\n",
    "            \"video_url\": video_url,\n",
    "            \"transcript\": all_phrases,\n",
    "            \"transcript_text\": full_text\n",
    "        }\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} • Full Transcript\",\n",
    "            \"content_text\": json.dumps(full_obj, ensure_ascii=ensure_ascii)\n",
    "        })\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c63e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the Video URL\n",
    "video_url = f\"{storage_account_endpoint}/{storage_container_name}/BMW_circularity.mp4\"\n",
    "\n",
    "# Analyzing the video and building the dataset\n",
    "video_analysis_result = analyze_video(video_url)\n",
    "video_dataset = build_simple_video_dataset(video_analysis_result, video_url, title=\"BMW_circularity_video\")\n",
    "\n",
    "# print the dataset in a pretty format\n",
    "print(json.dumps(video_dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218878dc",
   "metadata": {},
   "source": [
    "### Creating Functions for Performing Audio Analysis using Prebuilt Audio Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89451888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to analyze audio using prebuilt audio analyzer - makes API calls to Content Understanding service\n",
    "def analyze_audio(audio_url):\n",
    "    prebuilt_audio_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-audioAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "    body = {\n",
    "        \"url\": audio_url\n",
    "    }\n",
    "\n",
    "    audio_analysis_result = {}\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                    \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "                }\n",
    "\n",
    "        response = requests.post(prebuilt_audio_analyzer_url, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        analysis_id = result.get(\"id\")\n",
    "        print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "        # Using the analysis ID to get results; polling until the analysis is complete\n",
    "        get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "        }\n",
    "        analysis_status = \"Running\"\n",
    "        while analysis_status == \"Running\":\n",
    "            status_response = requests.get(get_result_url, headers=headers)\n",
    "            status_response.raise_for_status()\n",
    "            status_result = status_response.json()\n",
    "            analysis_status = status_result.get(\"status\")\n",
    "            print(\"Current Analysis Status:\", analysis_status)\n",
    "            if analysis_status == \"Running\":\n",
    "                import time\n",
    "                time.sleep(20)  # Wait before polling again\n",
    "        result_response = requests.get(get_result_url, headers=headers)\n",
    "        result_response.raise_for_status()\n",
    "        audio_analysis_result = result_response.json()\n",
    "        print(\"Audio Analysis Result:\", audio_analysis_result)\n",
    "        return audio_analysis_result\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Creating a Function to extract relevant information from the audio analysis result and prepare it for RAG\n",
    "def build_simple_audio_dataset(analysis_result, audio_url, title, include_full_transcript_row=True):\n",
    "    \"\"\"\n",
    "    Create simple dataset rows with document_title + content_text from audio analysis.\n",
    "    \"\"\"\n",
    "    result = (analysis_result or {}).get(\"result\", {}) or {}\n",
    "    contents = result.get(\"contents\") or []\n",
    "\n",
    "    rows = []\n",
    "    all_phrases = []\n",
    "\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        st, et = item.get(\"startTimeMs\"), item.get(\"endTimeMs\")\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "        phrases = item.get(\"transcriptPhrases\") or []\n",
    "        all_phrases.extend(phrases)\n",
    "\n",
    "        # Optional summary field (if present)\n",
    "        fields = item.get(\"fields\") or {}\n",
    "        summary = ((fields.get(\"Summary\") or {}).get(\"valueString\")) if fields else None\n",
    "\n",
    "        # Short transcript excerpt\n",
    "        transcript_excerpt = \"\"\n",
    "        if phrases:\n",
    "            transcript_excerpt = \" \".join((p.get(\"text\") or \"\").strip() for p in phrases[:3]).strip()\n",
    "\n",
    "        content_text = json.dumps({\n",
    "            \"about\": \"This is a JSON string representing a slice of audio analysis for RAG.\",\n",
    "            \"audio_url\": audio_url,\n",
    "            \"content_index\": idx,\n",
    "            \"kind\": kind,\n",
    "            \"time_window\": {\"start_ms\": st, \"end_ms\": et},\n",
    "            \"analyzer_markdown_excerpt\": md[:400],\n",
    "            \"transcript_excerpt\": transcript_excerpt,\n",
    "            **({\"summary\": summary} if summary else {})\n",
    "        }, ensure_ascii=False)\n",
    "\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} • Segment {idx}\",\n",
    "            \"content_text\": content_text\n",
    "        })\n",
    "\n",
    "    # Optionally add a full transcript row for global search\n",
    "    if include_full_transcript_row and all_phrases:\n",
    "        full_transcript_text = \" \".join((p.get(\"text\") or \"\").strip() for p in all_phrases).strip()\n",
    "        full_obj = {\n",
    "            \"about\": \"This is a JSON string representing the full audio transcript.\",\n",
    "            \"audio_url\": audio_url,\n",
    "            \"transcript_text\": full_transcript_text,\n",
    "            \"transcript\": [\n",
    "                {\n",
    "                    \"speaker\": p.get(\"speaker\"),\n",
    "                    \"start_ms\": p.get(\"startTimeMs\"),\n",
    "                    \"end_ms\": p.get(\"endTimeMs\"),\n",
    "                    \"text\": p.get(\"text\"),\n",
    "                    \"confidence\": p.get(\"confidence\")\n",
    "                } for p in all_phrases\n",
    "            ]\n",
    "        }\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} • Full Transcript\",\n",
    "            \"content_text\": json.dumps(full_obj, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    # Fallback if nothing produced\n",
    "    if not rows:\n",
    "        content_text = json.dumps({\n",
    "            \"about\": \"This is a JSON string representing a generic audio analysis record.\",\n",
    "            \"audio_url\": audio_url,\n",
    "            \"note\": \"No content segments found in analysis.\"\n",
    "        }, ensure_ascii=False)\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} • Full Audio\",\n",
    "            \"content_text\": content_text\n",
    "        })\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a1a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the Audio URL\n",
    "audio_url = f\"{storage_account_endpoint}/{storage_container_name}/BMW_forwardism.mp3\"\n",
    "\n",
    "# Analyzing the audio and building the dataset\n",
    "audio_analysis_result = analyze_audio(audio_url)\n",
    "audio_dataset = build_simple_audio_dataset(audio_analysis_result, audio_url, title=\"BMW_forwardism_audio\")\n",
    "\n",
    "# print the dataset in a pretty format\n",
    "print(json.dumps(audio_dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6b657",
   "metadata": {},
   "source": [
    "### Creating Functions for Performing Image Analysis using Prebuilt Image Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7674e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to analyze image using prebuilt image analyzer - makes API calls to Content Understanding service\n",
    "def analyze_image(image_url):\n",
    "    prebuilt_image_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-imageAnalyzer:analyze?api-version=2025-05-01-preview\"\n",
    "\n",
    "    body = {\n",
    "        \"url\": image_url\n",
    "    }\n",
    "\n",
    "    image_analysis_result = {}\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                    \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "                }\n",
    "\n",
    "        response = requests.post(prebuilt_image_analyzer_url, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        analysis_id = result.get(\"id\")\n",
    "        print(\"Analysis ID:\", analysis_id)\n",
    "\n",
    "        # Using the analysis ID to get results; polling until the analysis is complete\n",
    "        get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version=2025-05-01-preview\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "        }\n",
    "        analysis_status = \"Running\"\n",
    "        while analysis_status == \"Running\":\n",
    "            status_response = requests.get(get_result_url, headers=headers)\n",
    "            status_response.raise_for_status()\n",
    "            status_result = status_response.json()\n",
    "            analysis_status = status_result.get(\"status\")\n",
    "            print(\"Current Analysis Status:\", analysis_status)\n",
    "            if analysis_status == \"Running\":\n",
    "                import time\n",
    "                time.sleep(1)  # Wait before polling again\n",
    "        result_response = requests.get(get_result_url, headers=headers)\n",
    "        result_response.raise_for_status()\n",
    "        image_analysis_result = result_response.json()\n",
    "        print(\"Image Analysis Result:\", image_analysis_result)\n",
    "        return image_analysis_result\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Creating a Function to extract relevant information from the image analysis result and prepare it for RAG\n",
    "def build_simple_image_dataset(analysis_result, image_url, title):\n",
    "    \"\"\"\n",
    "    Create simple dataset rows with document_title + content_text from image analysis.\n",
    "    Pulls 'Summary' from fields if present.\n",
    "    \"\"\"\n",
    "    result = (analysis_result or {}).get(\"result\", {}) or {}\n",
    "    contents = result.get(\"contents\") or []\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "        fields = item.get(\"fields\") or {}\n",
    "\n",
    "        # Extract a clean summary string if present\n",
    "        summary_obj = fields.get(\"Summary\") or {}\n",
    "        summary_text = summary_obj.get(\"valueString\")\n",
    "\n",
    "        payload = {\n",
    "            \"about\": \"This is a JSON string representing a slice of image analysis for RAG.\",\n",
    "            \"image_url\": image_url,\n",
    "            \"content_index\": idx,\n",
    "            \"kind\": kind,\n",
    "            \"analyzer_markdown_excerpt\": md[:400],\n",
    "        }\n",
    "        if summary_text:\n",
    "            payload[\"summary\"] = summary_text\n",
    "        else:\n",
    "            # If you prefer preserving all fields, keep them—but they’re verbose\n",
    "            payload[\"fields_raw\"] = fields\n",
    "\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} • Content {idx}\",\n",
    "            \"content_text\": json.dumps(payload, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} • Full Image\",\n",
    "            \"content_text\": json.dumps({\n",
    "                \"about\": \"This is a JSON string representing a generic image analysis record.\",\n",
    "                \"image_url\": image_url,\n",
    "                \"note\": \"No content found in analysis.\"\n",
    "            }, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7360fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the Image URL\n",
    "image_url = f\"{storage_account_endpoint}/{storage_container_name}/image.png\"\n",
    "\n",
    "# Analyzing the image and building the dataset\n",
    "image_analysis_result = analyze_image(image_url)\n",
    "image_dataset = build_simple_image_dataset(image_analysis_result, image_url, title=\"Sample Image\")\n",
    "\n",
    "# print the dataset in a pretty format\n",
    "print(json.dumps(image_dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a813f",
   "metadata": {},
   "source": [
    "### Extracting Textual Data from PDF Document and Preparing it for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "\n",
    "def _is_url(s: str) -> bool:\n",
    "    return s.lower().startswith((\"http://\", \"https://\"))\n",
    "\n",
    "def _download_bytes(url: str) -> bytes:\n",
    "    r = requests.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def _clean_text(text: str) -> str:\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_pdf_text(pdf_source: str):\n",
    "    \"\"\"\n",
    "    Extracts plain text from each page of a PDF.\n",
    "    pdf_source can be a local file path or a URL.\n",
    "    Returns: list of {\"pageNumber\": int, \"text\": str}\n",
    "    \"\"\"\n",
    "    if _is_url(pdf_source):\n",
    "        data = _download_bytes(pdf_source)\n",
    "        fp = io.BytesIO(data)\n",
    "    else:\n",
    "        fp = open(pdf_source, \"rb\")\n",
    "\n",
    "    pages = []\n",
    "    try:\n",
    "        output = io.StringIO()\n",
    "        laparams = LAParams()\n",
    "        extract_text_to_fp(fp, output, laparams=laparams, output_type=\"text\")\n",
    "        full_text = output.getvalue()\n",
    "        raw_pages = full_text.split(\"\\x0c\")  # pdfminer page delimiter\n",
    "        for i, txt in enumerate(raw_pages, start=1):\n",
    "            cleaned = _clean_text(txt)\n",
    "            if i == len(raw_pages) and not cleaned:\n",
    "                continue\n",
    "            pages.append({\"pageNumber\": i, \"text\": cleaned})\n",
    "    finally:\n",
    "        fp.close()\n",
    "    return pages\n",
    "\n",
    "\n",
    "def build_simple_pdf_dataset(pages, pdf_source: str, title: str):\n",
    "    \"\"\"\n",
    "    Convert PDF pages into dataset rows like your audio/video/image functions.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for page in pages:\n",
    "        pno = page[\"pageNumber\"]\n",
    "        text = page[\"text\"]\n",
    "\n",
    "        payload = {\n",
    "            \"about\": \"This is a JSON string representing a slice of PDF for RAG.\",\n",
    "            \"pdf_source\": pdf_source,\n",
    "            \"page_number\": pno,\n",
    "            \"page_text\": text\n",
    "        }\n",
    "\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} • Page {pno}\",\n",
    "            \"content_text\": json.dumps(payload, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe102ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the PDF URL\n",
    "pdf_url = f\"{storage_account_endpoint}/{storage_container_name}/BMW_sustainable_natural_rubber.pdf\"\n",
    "\n",
    "# Extracting text from the PDF and building the dataset\n",
    "extracted_text = extract_pdf_text(pdf_url)\n",
    "pdf_dataset = build_simple_pdf_dataset(extracted_text, pdf_url, title=\"BMW Sustainable Natural Rubber PDF\")\n",
    "\n",
    "print(json.dumps(pdf_dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e0b9b",
   "metadata": {},
   "source": [
    "### Combining Datasets for Multi-Modal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = video_dataset + audio_dataset + image_dataset + pdf_dataset\n",
    "print(f\"Total dataset rows: {len(final_dataset)}\")\n",
    "\n",
    "print(json.dumps(final_dataset, indent=2))  # print first 2 rows as a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbeaea",
   "metadata": {},
   "source": [
    "### Creating an Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd71319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_AI_API_KEY\"),\n",
    "    api_version = \"2024-02-15-preview\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40fbe8c",
   "metadata": {},
   "source": [
    "### Creating a Function for Generating Embeddings using Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230ac057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "def generate_embeddings(text, embedding_model_name):\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=embedding_model_name\n",
    "    )\n",
    "\n",
    "    embeddings = response.model_dump()\n",
    "\n",
    "    return embeddings[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08683f41",
   "metadata": {},
   "source": [
    "### Finalising RAG Dataset with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee661fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "for data in final_dataset:\n",
    "    embedding = generate_embeddings(data[\"content_text\"], os.getenv(\"EMBEDDING_MODEL_NAME\"))\n",
    "    data[\"content_embedding\"] = embedding\n",
    "    data[\"content_id\"] = uuid.uuid4().hex\n",
    "\n",
    "# printing the final dataset with embeddings in a file \n",
    "with open(\"data.json\", \"w\") as f:\n",
    "    f.write(json.dumps(final_dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6695e7",
   "metadata": {},
   "source": [
    "### Creating our Azure AI Search Index in Azure Search Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a59541",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_service_endpoint = os.getenv(\"SEARCH_SERVICE_ENDPOINT\").strip().rstrip('/')\n",
    "search_service_api_key = os.getenv(\"SEARCH_SERVICE_API_KEY\")\n",
    "\n",
    "index_creation_url = search_service_endpoint +\"/indexes?api-version=2025-08-01-preview\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": search_service_api_key\n",
    "}\n",
    "\n",
    "# reading the index schema from index.json file\n",
    "with open(\"index.json\", \"r\") as f:\n",
    "    index_schema = json.load(f)\n",
    "\n",
    "body = index_schema \n",
    "\n",
    "response = requests.post(index_creation_url, headers=headers, json=body)\n",
    "\n",
    "if response.status_code == 201:\n",
    "    print(\"Index created successfully.\")\n",
    "    print(\"Response body:\", response.json())\n",
    "else:\n",
    "    print(\"Error creating index:\", response.status_code, response.text)\n",
    "    print(\"Response body:\", response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe61463",
   "metadata": {},
   "source": [
    "### Preparing the RAG Data for Bulk Upload to Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for object in dataset:\n",
    "    object[\"@search.action\"] = \"upload\" # append the action to each object\n",
    "\n",
    "with open(\"data_with_actions.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d4cb4",
   "metadata": {},
   "source": [
    "### Pushing RAG formatted data to Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de81611",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_with_actions.json\", \"r\") as f:\n",
    "    rag_dataset = json.load(f)\n",
    "\n",
    "\n",
    "bulk_upload_url = search_service_endpoint +\"/indexes/multi-modal-rag-index/docs/index?api-version=2025-08-01-preview\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": search_service_api_key\n",
    "}\n",
    "\n",
    "body = {\n",
    "    \"value\": [*rag_dataset]\n",
    "}\n",
    "\n",
    "response = requests.post(bulk_upload_url, headers=headers, json=body)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Bulk upload successful.\")\n",
    "    print(\"Response body:\", response.json())\n",
    "else:\n",
    "    print(\"Error during bulk upload:\", response.status_code, response.text)\n",
    "    print(\"Response body:\", response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
